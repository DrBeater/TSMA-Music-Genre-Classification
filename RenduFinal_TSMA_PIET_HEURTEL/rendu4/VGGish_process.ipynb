{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Jk5BPzkmkyl",
        "outputId": "a8a9c35a-c947-4f1b-eb7a-a051c99e6054"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "'!pip install soundfile\\n\\n!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_adapte.csv\\n!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_head.csv\\n!wget http://dept-info.labri.fr/~hanna/ProjetClassif/train_clean.csv'"
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"!pip install soundfile\n",
        "\n",
        "!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_adapte.csv\n",
        "!wget http://dept-info.labri.fr/~hanna/ProjetClassif/features_head.csv\n",
        "!wget http://dept-info.labri.fr/~hanna/ProjetClassif/train_clean.csv\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "DDXtpEgF9vIs",
        "outputId": "0ef1b7cf-9347-466b-801e-f00d436343f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": "\"\\nfor f in (files) : \\n  emb = embedding_from_fn(f)\\n  print('taille embed : ', emb.shape)\\n  dataset.append([emb[0:295]]) # limit if required\\n\\n# pickles\\n\""
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import librosa, librosa.display\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import soundfile as sf\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import librosa\n",
        "import pandas as pd\n",
        "import os\n",
        "from os.path import isfile, join\n",
        "from os import listdir\n",
        "\n",
        "# Load the model.\n",
        "vggmodel = hub.load('https://tfhub.dev/google/vggish/1')\n",
        "\n",
        "def embedding_from_fn(fn):\n",
        "  x, sr = librosa.load(fn) #,sr=None\n",
        "  x_16k = librosa.resample(x,sr,16000) #resample to 16KHz\n",
        "  embedding = np.array(vggmodel(x_16k)) \n",
        "  return embedding\n",
        "\n",
        "dataset=[]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WTl-GkxVM17h"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-12-31 10:06:14.817746: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "test over -> train\n",
            "over\n"
          ]
        }
      ],
      "source": [
        "test_dataset = []\n",
        "PATH_TESTS = './test/'\n",
        "fichiers_test = [f for f in listdir(PATH_TESTS) if isfile(join(PATH_TESTS, f))]\n",
        "#for f in df['track_id'] :\n",
        "for f in fichiers_test :\n",
        "  emb = embedding_from_fn(PATH_TESTS + f)\n",
        "  #print('taille embed : ', emb.shape)\n",
        "  test_dataset.append([emb[0:295]]) # limit if required\"\"\"\n",
        "# pickles\n",
        "\n",
        "with open('test_dataset.pickle', 'wb') as output:\n",
        "    pickle.dump(test_dataset, output)\n",
        "\n",
        "\n",
        "train_dataset = []\n",
        "PATH_TRAIN = './train/'\n",
        "fichiers_train = [f for f in listdir(PATH_TRAIN) if isfile(join(PATH_TRAIN, f))]\n",
        "#for f in df['track_id'] :\n",
        "for f in fichiers_train :\n",
        "  emb = embedding_from_fn(PATH_TRAIN + f)\n",
        "  #print('taille embed : ', emb.shape)\n",
        "  train_dataset.append([emb[0:295]]) # limit if required\"\"\"\n",
        "# pickles\n",
        "\n",
        "with open('train_dataset.pickle', 'wb') as output:\n",
        "    pickle.dump(train_dataset, output)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Copie de TSMA_VGGish_process.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}